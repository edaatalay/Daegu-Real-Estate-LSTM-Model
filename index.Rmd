---
title: "DEEP LEARNING ALGORITHMS FOR TIME SERIES ANALYSIS WITH APPLICATIONS"
author: "Eda Atalay & İpek Korkmaz"
date: "16/01/2022"
output:
  html_document: default
---
```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

### 1. DEFINITION AND PURPOSE OF THE STUDY

The goal of this study is to achieve fundamental and technical information about deep learning while researching the literature and applying it on code practices. 

### 2. AN OVERVIEW OF DEEP LEARNING

The history of deep learning goes back to McCulloch & Pitts paper published in 1943. According to the study, what makes a human brain a computational device is the neural activity in the mind. Taking this into consideration, deep learning algorithms have been influenced by human brains, which can understand complicated information.

If a network has enough number of neurons and has a proper synaptic connection, then it can compute any value. In this case, a simple logic function will be applied depending on the weights in the McCulloch & Pitts neuron. Furthermore, the concept of threshold is an important feature. For a precise neuron, if the net input which is weighted sum of the inputs are greater than the certain threshold then the neuron will be fired. 

In 1949, Hebb published a book called “The Organization of Behavior”. Hebb suggested that the brain's connectivity is constantly changing as an organism while learning different functional tasks. The idea behind Hebb's theory is that if two neurons are found to be active at the same time, fired together, the strength of the connection between them should be increased. The strength of the connection between neurons will be changed while learning.

Tasks such as learning to classify labeled examples and recognition of distinct patterns were performed by the term called perceptron. Perceptron is the method for iterative weight arrangement, therefore the weights on the connection paths can be improved. Rosenblatt proved a theorem that the learning algorithm can find the right answer if there was a sufficient set of parameters for classifying and there were adequate number of examples. However, in the 1950s the perceptron learning algorithm was needed digital computers to compute with real numbers which was carried out insufficiently in that time. 

Deep learning networks allows communication between computers and human beings while being a bridge between digital and the daily life. In other words, it provides a balance between real word which is complex and indefinite with a word with symbols and rules.  

### 3. NEURAL NETWORKS  
In this section, we closely followed the book titled “An Introduction to Statistical Learning with Application in R” by Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani.  

#### 3.1. Single Layer Neural Networks  
For predicting the response Y, a neural network will build nonlinear function $\vec{X} \to Y$ where $\small X$ is input vector of $\small n$ number of variables $\vec{X} =(X_1,X_2,...,Xn)$. These input units will affect the model’s result.  

<center>![_Figure 1: Neural network with single layer_](images/picture1.jpg)</center>  

_Figure 1_ shows feed-forward neural network with single layer. The input vector $\vec{X}$ will create input layer neurons in the neural network. Each unit from the input layer will feed the hidden layer network which has K hidden units. In this layer, the units are neither inputs nor outputs, that is why it is called hidden.  
$$f(X)=β_0+\sum_{k=1}^Kβ_k h_k (X) $$  
$$=β_0+\sum_{k=1}^Kβ_k g(w_0k+\sum_{i=1}^nw_{ik} X_i).$$
In Equations above, $\small w_{11},…,w_{nK} $ indicate weights flow through input layer to hidden layer. Similarly,$\small β_1,β_2,…,β_K$ indicate weights flow through hidden layer to output. $\small β_0$ and $\small w_{0k}$ are the bias terms. First index of $\small w$ shows that which neuron in the input layer will feed into the hidden layer. Second index of $\small w$ shows which neuron in the hidden layer feeded up.

$\smallβ$  and $\small w$ parameters must be estimated from data. 

##### 3.1.1. Activation Function 

Nonlinearity is included to a neuron through activation function that is crucial for finding a solution of complex neural network problems. If nonlinearity in the activation function is not applied, network model turns into a simple linear regression model. Also, the nonlinearity of the activation function provides better reflection of complex data.

$\small A_k$ are the neurons in the hidden layer.$\small h_k (X) $is the function that builds $\small A_k$ with input features. $\small g(t)$ is an activation function that is nonlinear in advance:  
$$A_k= h_k(X)=g(w_{0k}+\sum_{i=1}^nw_{ik} X_i).$$
$\small f(X)$  function will be computed with weights $\small β_k$ and the activations $\small A_k$ from hidden layer pass into the output layer: 

$$f(X)= β_0+\sum_{k=1}^Kβ_k A_{k}. $$  

##### 3.1.1.1. Sigmoid Activation Function

Sigmoid activation function was used as a default activation function for neural network until the early 1990s. It scales input values between 0 and 1,  
$$g(z)=  \frac{e^z}{(1+ e^z )}=  \frac{1}{(1+ e^{-z})}.$$  
Only changes around the mid-point are sensitive to the functions as seen in _Figure 2_.  

##### 3.1.1.2. ReLU Activation Function

The rectified linear activation function (ReLU) is a piecewise function that when input value is positive it equals to itself, or else it equals to 0. If the input value is positive ReLU function is linear, or else it is nonlinear. The ReLU function is more sensitive than sigmoid function.  
\begin{equation}
g(z)=(z)_{+}= 
\begin{cases} 
0         & \text{if } z < 0 \\
z         & \text{otherwise } 
\end{cases}
\end{equation}
<center>![_Figure 2:Sigmoid and ReLU activation functions, respectively_](images/picture2.jpg)</center>  
Earlier, sigmoid activation function was preferred, but today, ReLU activation function is common in modern neural networks because it is more powerful to compute and store.   

##### 3.1.2. Squared-Error Loss

Squared error function is commonly used to estimate efficiency of the neural network model. It is used to compare the predicted value with the actual value where $\small f(x_i)$ is the prediction and $\small y_i$ is the $\small i^{th}$ observation. The unknown parameters such as biases and weights are determined to minimize

$$\sum_{i=1}^n(y_i-f(x_i ))^2,$$  
while fitting the model.  

#### 3.2. Multilayer Neural Networks

A large size single hidden layer is theoretically sufficient to make an acceptable approximation. Nevertheless, more than one hidden layer with small size returns good results much more easily.  
<center>![_Figure 3: Neural network with two hidden layers, p number of outputs and weight matrices $\small W_1$ ,$\small W_2$,B_.](images/picture3.jpg)</center>  

Apart from the number of layers, the difference between _Figure 1_ and _Figure 3_ is the number of results. In single layer neural network there were one output, however in this example, in two hidden layer neural network there are p number of outputs. Nevertheless, it is not necessary to have multiple outputs.

Similarly to single layer neural network, the activation function applied from input layer to first hidden layer:

$$A_k^{(1)}  = h_k^{(1)}(X) = g(w_{0k}^{(1)}+\sum_{i=1}^nw_{ik}^{(1)} X_i), $$  
for $\small k=1,2,…,K_1.$  

First hidden layer activations $\small A_k^{(1)} $ will be treated as inputs for the second hidden layer activations:   
$$A_l^{(2)}  = h_l^{(2)}(X) = g(w_{0l}^{(2)}+\sum_{k=1}^{K_1}w_{kl}^{(2)} A_k^{(1)})$$  
for $\small l=1,2,…,K_2.$  

Each of the activation functions, are function of $\small \vec{X}$ input vector. After sequences of transformations, the network will generate complex $\small \vec{X}$ transformations which are fed as features into the output layer.  

At the output layer there are p number of results. The function is 
$$f_m (X)=β_{0m}+\sum_{l=1}^{K_2}β_{lm}h_l^{(2)}(X)=β_{0m}+\sum_{l=1}^{K_2}β_{lm}A_l^{(2)},$$
while $\small m=1,2,…,p$ if all these results are different.  

### 4. CODE PRACTICES  

#### 4.1. The Libraries Used
 
The Python libraries showed in the _Table 1_, are essential to be able to use particular functions.  

<style>
table {
  text-align:center;
  font-family: arial, sans-serif;
  border: 1px solid black;
  border-collapse: collapse;
  width: 100%;
}

td{
  text-align:left;
  border: 1px solid black;
  padding: 8px;
}
th {
  text-align:center;
  border: 1px solid black;
  padding: 8px;
}
tr:nth-child(even) {
  background-color: #dddddd;
}
</style>
<table class="center" >
  <tr>
    <th>Libraries      </th>
    <th>Description</th>
  </tr>
  <tr>
    <td>NumPy</td>
    <td>NumPy is the essential package for scientific computing such as linear algebra.</td>
  </tr>
  <tr>
    <td>pandas</td>
    <td>Pandas is an open-source data analysis and manipulation tool built on the Python programming language.</td>
  </tr>
  <tr>
    <td>tensorflow</td>
    <td>TensorFlow is an open-source library which is used mainly for deep learning applications.</td>
  </tr>
  <tr>
    <td>sklearn</td>
    <td>Sklearn library is essential for machine learning applications and statistical modeling in term of tools for classification and regression.</td>
  </tr>
  <tr>
    <td>keras</td>
    <td>Keras which runs on top of the TensorFlow is used for evaluating deep learning models such as artificial neural networks.</td>
  </tr>
  <tr>
    <td>matplotlib</td>
    <td>Matplotlib is useful library for data visualization and graphical plotting.</td>
  </tr>
</table>
<center><p> _Table 1: Python libraries used._ </p></center> 

```{r, eval=F, echo=T}
import numpy as np
import matplotlib.pyplot as plt  import pandas as pd
from pandas import Series from tensorflow.keras.models import Sequential from tensorflow.keras.layers import Dense
from sklearn.model_selection import train_test_split from keras.callbacks import EarlyStopping
from sklearn.preprocessing import MinMaxScaler
```

#### 4.2. The Dataset

The dataset is taken from [Kaggle](https://www.kaggle.com/gunhee/koreahousedata).

```{r, eval=F, echo=T}
daegu_df = pd.read_csv("Daegu_Real_Estate_data.csv")
```
  
The data called Daegu_Real_Estate_data is in CSV format. The dataset contains house sale prices in the city of Daegu, Republic of Korea. 

The dataset has 5891 rows and 30 columns. This dataset is chosen because it is suitable for the regression problem and has large number of entries without NA values. Moreover, due to the large number of columns, the result depended on many different variables. 

Several variables are considered as less relevant to the house sale prices. For this reason, some columns are dropped. 

After dropping,  there are only 19 columns are remained as shown in _Table 2_. For the regression problem, the target variable is chosen as “SalePrice”.





